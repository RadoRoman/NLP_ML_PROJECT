{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57f8de54",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dharm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\dharm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "from gensim.models import LsiModel\n",
    "from gensim import corpora\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from sklearn.decomposition import TruncatedSVD\n",
    "#from sklearn.pipeline import Pipeline\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "#from dataclean import importing_Lyrics_df, pre_processing\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "520ccd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importing_Lyrics_df():\n",
    "    df = pd.read_csv('C:/Users/dharm/OneDrive - IMC/FH Krems/4th sem/datascience capstone/NLP_ML_PROJECT/data/raw/azlyrics_lyrics_19.csv',on_bad_lines='skip')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8888e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(text):\n",
    "    stop_list = []\n",
    "    stop_list.extend(stopwords.words(['hungarian','swedish','kazakh','norwegian','finnish','arabic','indonesian','portuguese','turkish','azerbaijani', 'slovene', 'spanish', 'danish', 'nepali', 'romanian', 'greek', 'dutch', 'README', 'tajik', 'german', 'english', 'russian', 'french', 'italian']))\n",
    "    stop_list.extend(\n",
    "        ['yo','dont','nigga','uh', 'got', 'oh', 'im', 'na', 'from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be',\n",
    "         'know', 'good', 'go', 'get', 'ah', 'bout','yeah','le','ayy','u','bitch','eh','wa',\n",
    "         'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot',\n",
    "         'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])\n",
    "    exclude = set(string.punctuation)\n",
    "    lemma = WordNetLemmatizer()\n",
    "\n",
    "    text = text.rstrip()\n",
    "    normalized = \" \".join(lemma.lemmatize(i) for i in text.split())\n",
    "    punc_free = ''.join(i for i in normalized if i not in exclude)\n",
    "    stop_free = \" \".join([i for i in punc_free.lower().split() if((i not in stop_list) and (not i.isdigit()))])\n",
    "    return stop_free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc805a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus(doc_clean):\n",
    "    \"\"\"\n",
    "    Input  : clean document\n",
    "    Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Document Term Matrix\n",
    "    Output : term dictionary and Document Term Matrix\n",
    "    \"\"\"\n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    # generate LDA model\n",
    "    return dictionary,doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cef540c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gensim_lsa_model(doc_clean,number_of_topics,words):\n",
    "    \"\"\"\n",
    "    Input  : clean document, number of topics and number of words associated with each topic\n",
    "    Purpose: create LSA model using gensim\n",
    "    Output : return LSA model\n",
    "    \"\"\"\n",
    "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "    # generate LSA model\n",
    "    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "    print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
    "    return lsamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f7d5c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Input   : dictionary : Gensim dictionary\n",
    "              corpus : Gensim corpus\n",
    "              texts : List of input texts\n",
    "              stop : Max num of topics\n",
    "    purpose : Compute c_v coherence for various number of topics\n",
    "    Output  : model_list : List of LSA topic models\n",
    "              coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, stop, step):\n",
    "        # generate LSA model\n",
    "        model = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74c198e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(doc_clean,start, stop, step):\n",
    "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "    model_list, coherence_values = compute_coherence_values(dictionary, doc_term_matrix,doc_clean,\n",
    "                                                            stop, start, step)\n",
    "    # Show graph\n",
    "    x = range(start, stop, step)\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherence_values\"), loc='best')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4915b579",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = importing_Lyrics_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bf6c0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clean=[]\n",
    "for text in df['LYRICS']:\n",
    "    text_clean.append(pre_processing(text).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5271aa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary,corpus = prepare_corpus(text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3c69a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.491*\"love\" + 0.452*\"like\" + 0.235*\"cant\" + 0.173*\"touch\" + 0.165*\"stop\" + 0.148*\"falling\" + 0.134*\"youre\" + 0.133*\"baby\" + 0.120*\"aint\" + 0.108*\"wanna\"'), (1, '-0.579*\"already\" + -0.579*\"gone\" + 0.345*\"love\" + -0.196*\"neo\" + 0.172*\"touch\" + -0.157*\"away\" + 0.133*\"falling\" + -0.091*\"ijen\" + -0.091*\"youre\" + 0.079*\"cuz\"'), (2, '-0.520*\"love\" + 0.463*\"like\" + -0.273*\"touch\" + -0.250*\"already\" + -0.248*\"gone\" + -0.224*\"falling\" + 0.177*\"cant\" + 0.129*\"stop\" + -0.114*\"cuz\" + 0.112*\"aint\"'), (3, '0.460*\"gonna\" + 0.344*\"wanna\" + -0.300*\"like\" + -0.217*\"stop\" + -0.215*\"cant\" + -0.195*\"touch\" + 0.195*\"baby\" + -0.182*\"falling\" + 0.145*\"youre\" + 0.135*\"hot\"'), (4, '0.371*\"cant\" + 0.344*\"stop\" + -0.275*\"touch\" + -0.258*\"issa\" + -0.244*\"falling\" + 0.218*\"hey\" + 0.173*\"hot\" + -0.160*\"aint\" + 0.157*\"cuz\" + -0.151*\"ooh\"'), (5, '-0.858*\"issa\" + 0.193*\"touch\" + 0.186*\"falling\" + -0.162*\"cuz\" + -0.135*\"hey\" + -0.133*\"love\" + -0.129*\"hot\" + 0.094*\"youre\" + 0.080*\"gonna\" + -0.080*\"real\"'), (6, '0.421*\"gonna\" + 0.308*\"cant\" + -0.296*\"hot\" + 0.286*\"issa\" + 0.246*\"wanna\" + 0.215*\"stop\" + -0.208*\"issue\" + -0.193*\"cuz\" + 0.190*\"touch\" + 0.171*\"falling\"')]\n"
     ]
    }
   ],
   "source": [
    "number_of_topics=7\n",
    "words=10\n",
    "lsamodel=create_gensim_lsa_model(text_clean,number_of_topics,words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5584b1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lsamodel.print_topics(num_words= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67c1b89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_topic_details(lsamodel, corpus):\n",
    "    topic_details_df = pd.DataFrame()\n",
    "    for i, row in enumerate(lsamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = lsamodel.show_topic(topic_num)\n",
    "                topic_details_df = topic_details_df.append(pd.Series([topic_num, prop_topic]),\n",
    "                ignore_index=True)\n",
    "    topic_details_df.columns = ['Dominant_Topic', '% Score']\n",
    "    return topic_details_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7dbcde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = pd.DataFrame({'Original Text': text_clean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db2caacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dharm\\AppData\\Local\\Temp\\ipykernel_54548\\2080810723.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  topic_details_df = topic_details_df.append(pd.Series([topic_num, prop_topic]),\n",
      "C:\\Users\\dharm\\AppData\\Local\\Temp\\ipykernel_54548\\2080810723.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  topic_details_df = topic_details_df.append(pd.Series([topic_num, prop_topic]),\n"
     ]
    }
   ],
   "source": [
    "topic_details = pd.concat([get_topic_details(lsamodel, corpus), contents], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3855040c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Dominant_Topic    % Score  \\\n",
      "0             0.0  24.770249   \n",
      "1             0.0   5.763429   \n",
      "2             0.0   9.829807   \n",
      "3             0.0   7.947094   \n",
      "4             0.0   5.992174   \n",
      "\n",
      "                                       Original Text  flag  \n",
      "0  [grape, street, gslide, sweet, shay, shay, lik...     0  \n",
      "1  [gonna, put, suit, tie, steady, speakin, work,...     0  \n",
      "2  [kill, kill, radio, killer, music, dealer, son...     0  \n",
      "3  [lil, steady, speaking, low, project, wall, ta...     0  \n",
      "4  [x, loaded, bro, never, church, prayer, lost, ...     0  \n"
     ]
    }
   ],
   "source": [
    "dom_tp = topic_details['Dominant_Topic']\n",
    "topic_details['flag'] = np.where((dom_tp == 2.0) | (dom_tp == 3.0) | (dom_tp == 4.0), 1, 0)\n",
    "print(topic_details.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
