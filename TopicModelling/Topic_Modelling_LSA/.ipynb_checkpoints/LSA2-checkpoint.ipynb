{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dharm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\dharm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# data visualisation and manipulation\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "#configure\n",
    "# sets matplotlib to inline and displays graphs below the corressponding cell.\n",
    "%matplotlib inline  \n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid',color_codes=True)\n",
    "\n",
    "#import nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "\n",
    "#preprocessing\n",
    "from nltk.corpus import stopwords  #stopwords\n",
    "from nltk import word_tokenize,sent_tokenize # tokenizing\n",
    "from nltk.stem import PorterStemmer,LancasterStemmer  # using the Porter Stemmer and Lancaster Stemmer and others\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer # lammatizer from WordNet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "# for named entity recognition (NER)\n",
    "from nltk import ne_chunk\n",
    "\n",
    "# vectorizers for creating the document-term-matrix (DTM)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def importing_Lyrics_df():\n",
    "    df = pd.read_csv('C:/Users/dharm/OneDrive - IMC/FH Krems/4th sem/datascience capstone/NLP_ML_PROJECT/data/preprocessed/Lyrics.csv',on_bad_lines='skip')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list = []\n",
    "stop_list.extend(stopwords.words(['hungarian','swedish','kazakh','norwegian','finnish','arabic','indonesian','portuguese','turkish','azerbaijani', 'slovene', 'spanish', 'danish', 'nepali', 'romanian', 'greek', 'dutch', 'README', 'tajik', 'german', 'english', 'russian', 'french', 'italian']))\n",
    "stop_list.extend(['yo','dont','nigga','uh', 'got', 'oh', 'im', 'na', 'from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be',\n",
    "         'know', 'good', 'go', 'get', 'ah', 'bout','yeah','le','ayy','u','bitch','eh','wa',\n",
    "         'do', 'uuu','done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot',\n",
    "         'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(headline):\n",
    "  le=WordNetLemmatizer()\n",
    "  word_tokens=word_tokenize(headline)\n",
    "  tokens=[le.lemmatize(w) for w in word_tokens if w not in stop_list and len(w)>3]\n",
    "  cleaned_text=\" \".join(tokens)\n",
    "  return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = importing_Lyrics_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time taking\n",
    "df['Cleaned_LYRICS']=df['LYRICS'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARTIST_NAME</th>\n",
       "      <th>ARTIST_URL</th>\n",
       "      <th>SONG_NAME</th>\n",
       "      <th>SONG_URL</th>\n",
       "      <th>LYRICS</th>\n",
       "      <th>words</th>\n",
       "      <th>Cleaned_LYRICS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>03 greedo</td>\n",
       "      <td>https://www.azlyrics.com/19/03greedo.html</td>\n",
       "      <td>sweet lady</td>\n",
       "      <td>https://www.azlyrics.com/lyrics/03greedo/sweet...</td>\n",
       "      <td>its only one, 03, i'm from grape street, where...</td>\n",
       "      <td>['one', '03', 'im', 'grape', 'street', 'gslide...</td>\n",
       "      <td>grape street g-slide sweet shay shay like dark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03 greedo</td>\n",
       "      <td>https://www.azlyrics.com/19/03greedo.html</td>\n",
       "      <td>mafia business</td>\n",
       "      <td>https://www.azlyrics.com/lyrics/03greedo/mafia...</td>\n",
       "      <td>you gonna make me put you in a suit and tie, s...</td>\n",
       "      <td>['gonna', 'make', 'put', 'suit', 'tie', 'stead...</td>\n",
       "      <td>suit steady speakin 'bout work business busine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03 greedo</td>\n",
       "      <td>https://www.azlyrics.com/19/03greedo.html</td>\n",
       "      <td>paranoid</td>\n",
       "      <td>https://www.azlyrics.com/lyrics/03greedo/paran...</td>\n",
       "      <td>we could kill it, yeah, we could, we could, we...</td>\n",
       "      <td>['could', 'kill', 'it', 'yeah', 'radio', 'make...</td>\n",
       "      <td>kill kill radio killer music dealer song still...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>03 greedo</td>\n",
       "      <td>https://www.azlyrics.com/19/03greedo.html</td>\n",
       "      <td>never bend</td>\n",
       "      <td>https://www.azlyrics.com/lyrics/03greedo/never...</td>\n",
       "      <td>yeah, you lil bitch ass niggas steady speaking...</td>\n",
       "      <td>['yeah', 'lil', 'bitch', 'as', 'nigga', 'stead...</td>\n",
       "      <td>nigga steady speaking project wall talk like w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>03 greedo</td>\n",
       "      <td>https://www.azlyrics.com/19/03greedo.html</td>\n",
       "      <td>prayer for my lost</td>\n",
       "      <td>https://www.azlyrics.com/lyrics/03greedo/praye...</td>\n",
       "      <td>x loaded up bro, never see me in the church, p...</td>\n",
       "      <td>['x', 'loaded', 'bro', 'never', 'see', 'church...</td>\n",
       "      <td>loaded never church prayer lost beat boom pray...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ARTIST_NAME                                 ARTIST_URL           SONG_NAME  \\\n",
       "0   03 greedo  https://www.azlyrics.com/19/03greedo.html          sweet lady   \n",
       "1   03 greedo  https://www.azlyrics.com/19/03greedo.html      mafia business   \n",
       "2   03 greedo  https://www.azlyrics.com/19/03greedo.html            paranoid   \n",
       "3   03 greedo  https://www.azlyrics.com/19/03greedo.html          never bend   \n",
       "4   03 greedo  https://www.azlyrics.com/19/03greedo.html  prayer for my lost   \n",
       "\n",
       "                                            SONG_URL  \\\n",
       "0  https://www.azlyrics.com/lyrics/03greedo/sweet...   \n",
       "1  https://www.azlyrics.com/lyrics/03greedo/mafia...   \n",
       "2  https://www.azlyrics.com/lyrics/03greedo/paran...   \n",
       "3  https://www.azlyrics.com/lyrics/03greedo/never...   \n",
       "4  https://www.azlyrics.com/lyrics/03greedo/praye...   \n",
       "\n",
       "                                              LYRICS  \\\n",
       "0  its only one, 03, i'm from grape street, where...   \n",
       "1  you gonna make me put you in a suit and tie, s...   \n",
       "2  we could kill it, yeah, we could, we could, we...   \n",
       "3  yeah, you lil bitch ass niggas steady speaking...   \n",
       "4  x loaded up bro, never see me in the church, p...   \n",
       "\n",
       "                                               words  \\\n",
       "0  ['one', '03', 'im', 'grape', 'street', 'gslide...   \n",
       "1  ['gonna', 'make', 'put', 'suit', 'tie', 'stead...   \n",
       "2  ['could', 'kill', 'it', 'yeah', 'radio', 'make...   \n",
       "3  ['yeah', 'lil', 'bitch', 'as', 'nigga', 'stead...   \n",
       "4  ['x', 'loaded', 'bro', 'never', 'see', 'church...   \n",
       "\n",
       "                                      Cleaned_LYRICS  \n",
       "0  grape street g-slide sweet shay shay like dark...  \n",
       "1  suit steady speakin 'bout work business busine...  \n",
       "2  kill kill radio killer music dealer song still...  \n",
       "3  nigga steady speaking project wall talk like w...  \n",
       "4  loaded never church prayer lost beat boom pray...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect =TfidfVectorizer(stop_words=stop_list,max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dharm\\OneDrive - IMC\\FH Krems\\4th sem\\datascience capstone\\NLP_ML_PROJECT\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['100', '103', '112', '22', '49', '52', '83', 'ada', 'added', 'anoncvs', 'applications', 'arlc', 'augmented', 'azerbaijani', 'backend', 'baiknya', 'berkali', 'cgi', 'contains', 'corpus', 'corrected', 'cvsweb', 'english', 'frequency', 'german', 'github', 'grammatical', 'greek', 'high', 'http', 'https', 'ignored', 'indonesian', 'issues', 'kali', 'kazakh', 'kurangnya', 'language', 'languages', 'list', 'lists', 'mata', 'nepali', 'nltk', 'nltk_data', 'obtained', 'olah', 'onların', 'org', 'pgsql', 'postgresql', 'printr', 'pull', 'resources', 'retrieval', 'ro', 'romanian', 'sekurang', 'setidak', 'several', 'snowball', 'src', 'stop', 'stopwords', 'tama', 'text', 'tidaknya', 'usually', 'words', 'δι', 'арбаң', 'арсалаң', 'афташ', 'бай', 'бале', 'баски', 'батыр', 'баҳри', 'болои', 'бүгжең', 'бұтыр', 'валекин', 'вақте', 'вой', 'вуҷуди', 'гар', 'гарчанде', 'далаң', 'даме', 'ербелең', 'жалт', 'жұлт', 'карда', 'кошки', 'куя', 'күңгір', 'кӣ', 'магар', 'майлаш', 'митың', 'модоме', 'нияти', 'онан', 'оре', 'паһ', 'рӯи', 'салаң', 'сар', 'сұлаң', 'сұрт', 'тарбаң', 'тразе', 'ту', 'тыржың', 'тұрс', 'хом', 'хуб', 'чаро', 'чи', 'чун', 'чунон', 'шарте', 'шұңқ', 'ыржың', 'қадар', 'қайқаң', 'қалт', 'қаңғыр', 'қаңқ', 'қош', 'қызараң', 'құйқаң', 'құлт', 'құңқ', 'ұрс', 'ҳай', 'ҳамин', 'ҳатто', 'ҳо', 'ҳол', 'ҳолате', 'әттеген', 'ӯим', 'آمين', 'أب', 'أخ', 'أفعل', 'أفعله', 'ؤلاء', 'إل', 'إم', 'ات', 'اتان', 'ارتد', 'ان', 'انفك', 'برح', 'تان', 'تبد', 'تحو', 'تعل', 'حد', 'حم', 'حي', 'خب', 'ذار', 'سيما', 'صه', 'ظل', 'ظن', 'عد', 'قط', 'مر', 'مكان', 'مكانكن', 'نب', 'هات', 'هب', 'واها', 'وراء', 'अक', 'अग', 'अझ', 'अन', 'अर', 'आजक', 'आत', 'आद', 'आफ', 'आय', 'ईक', 'उद', 'उनक', 'उनल', 'उह', 'एउट', 'एन', 'कa', 'कत', 'कम', 'कस', 'कसर', 'कह', 'गत', 'गय', 'गर', 'चम', 'छन', 'जत', 'जबक', 'जस', 'जसक', 'जसब', 'जसम', 'जसल', 'जह', 'तत', 'तथ', 'तदन', 'तप', 'तवम', 'नज', 'नत', 'नभन', 'नय', 'पक', 'पछ', 'पन', 'पय', 'पर', 'पष', 'पह', 'बन', 'बर', 'भएक', 'भय', 'भव', 'मल', 'यत', 'यथ', 'यद', 'यप', 'यसक', 'यसपछ', 'यसब', 'यसर', 'यह', 'रण', 'रत', 'रमश', 'रह', 'लस', 'वर', 'सक', 'सट', 'सध', 'सपछ', 'सब', 'सम', 'सर', 'सह', 'हन', 'हर', 'हरण', 'ἀλλ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vect_text=vect.fit_transform(df['Cleaned_LYRICS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147892, 1000)\n",
      "  (0, 115)\t0.05785914576810423\n",
      "  (0, 963)\t0.14312222374620706\n",
      "  (0, 855)\t0.03348559029986347\n",
      "  (0, 344)\t0.04169405811784939\n",
      "  (0, 554)\t0.02949624554694575\n",
      "  (0, 715)\t0.03494468742754527\n",
      "  (0, 311)\t0.03655673549392283\n",
      "  (0, 37)\t0.1775644612377427\n",
      "  (0, 998)\t0.035860246142762724\n",
      "  (0, 941)\t0.03420328532137343\n",
      "  (0, 329)\t0.0473776768482653\n",
      "  (0, 964)\t0.03534510432022648\n",
      "  (0, 39)\t0.04855338089954928\n",
      "  (0, 366)\t0.04024057947508202\n",
      "  (0, 161)\t0.04685808543038492\n",
      "  (0, 486)\t0.03844618249413399\n",
      "  (0, 660)\t0.0592979166703351\n",
      "  (0, 679)\t0.03652738005780879\n",
      "  (0, 784)\t0.04372501566371418\n",
      "  (0, 637)\t0.0374900238834119\n",
      "  (0, 75)\t0.042877016286196404\n",
      "  (0, 173)\t0.0471688294745645\n",
      "  (0, 704)\t0.04612161651879203\n",
      "  (0, 172)\t0.044573096967191396\n",
      "  (0, 151)\t0.045554233169457\n",
      "  :\t:\n",
      "  (147890, 486)\t0.055268060838697786\n",
      "  (147890, 784)\t0.06285661329948393\n",
      "  (147890, 75)\t0.43146304097532534\n",
      "  (147890, 390)\t0.2063521460609232\n",
      "  (147890, 36)\t0.03168904649710556\n",
      "  (147890, 475)\t0.0977922976702017\n",
      "  (147891, 384)\t0.15080344472129487\n",
      "  (147891, 479)\t0.23041665854285567\n",
      "  (147891, 789)\t0.10743233558810357\n",
      "  (147891, 983)\t0.10088045830941351\n",
      "  (147891, 85)\t0.06785314896170615\n",
      "  (147891, 295)\t0.056755605188242256\n",
      "  (147891, 706)\t0.07333133728451094\n",
      "  (147891, 102)\t0.5482754679851972\n",
      "  (147891, 198)\t0.11846222734682932\n",
      "  (147891, 676)\t0.07108946586343376\n",
      "  (147891, 381)\t0.0485591042995055\n",
      "  (147891, 155)\t0.056973868443344844\n",
      "  (147891, 756)\t0.7205238308246782\n",
      "  (147891, 132)\t0.15876742876062272\n",
      "  (147891, 680)\t0.06185382718515828\n",
      "  (147891, 257)\t0.04278473681247796\n",
      "  (147891, 311)\t0.05995595348725247\n",
      "  (147891, 12)\t0.04510930882967196\n",
      "  (147891, 499)\t0.09110700212189941\n"
     ]
    }
   ],
   "source": [
    "print(vect_text.shape)\n",
    "print(vect_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf=vect.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like neoreul\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dharm\\OneDrive - IMC\\FH Krems\\4th sem\\datascience capstone\\NLP_ML_PROJECT\\venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "dd=dict(zip(vect.get_feature_names(), idf))\n",
    "l=sorted(dd, key=(dd).get)\n",
    "#print(l)\n",
    "print(l[0],l[-1])\n",
    "# like is most common and wey is least common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "lsa_model = TruncatedSVD(n_components=10, algorithm='randomized', n_iter=10, random_state=42)\n",
    "\n",
    "lsa_top=lsa_model.fit_transform(vect_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.38533787 -0.18589908  0.03560597 ...  0.01369723 -0.01193692\n",
      "   0.00397734]\n",
      " [ 0.21398012 -0.10543868  0.00491265 ... -0.04056792 -0.00544482\n",
      "  -0.00119003]\n",
      " [ 0.21716464 -0.14186529  0.00280316 ... -0.01418914  0.00288194\n",
      "   0.01611505]\n",
      " ...\n",
      " [ 0.17345482 -0.01975757  0.0336506  ... -0.08915622 -0.00540373\n",
      "   0.00727617]\n",
      " [ 0.16541288 -0.07822664 -0.00838423 ...  0.04883247  0.00615904\n",
      "   0.01037179]\n",
      " [ 0.11048834  0.04896815 -0.00701535 ...  0.00601529  0.00515175\n",
      "   0.00661334]]\n",
      "(147892, 10)\n"
     ]
    }
   ],
   "source": [
    "print(lsa_top)\n",
    "print(lsa_top.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 :\n",
      "Topic  0  :  38.53378656875574\n",
      "Topic  1  :  -18.58990799640928\n",
      "Topic  2  :  3.560596831006265\n",
      "Topic  3  :  17.96535927219321\n",
      "Topic  4  :  47.58569675091024\n",
      "Topic  5  :  -5.407032871242552\n",
      "Topic  6  :  -19.714725527756073\n",
      "Topic  7  :  1.3697228630161187\n",
      "Topic  8  :  -1.1936921625102794\n",
      "Topic  9  :  0.39773431728956987\n"
     ]
    }
   ],
   "source": [
    "l=lsa_top[0]\n",
    "print(\"Document 0 :\")\n",
    "for i,topic in enumerate(l):\n",
    "  print(\"Topic \",i,\" : \",topic*100)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1000)\n",
      "[[ 1.41740231e-02  7.53997172e-03  1.98395433e-02 ...  6.30274727e-03\n",
      "   3.30723866e-02  5.49848398e-03]\n",
      " [-7.51909334e-03 -6.22068374e-03 -1.55289380e-03 ... -6.34219253e-03\n",
      "  -2.50751414e-02 -6.37903549e-03]\n",
      " [-1.60271711e-03  6.97137290e-04 -3.45238881e-03 ...  3.89893012e-03\n",
      "   1.78359926e-04  1.09127255e-03]\n",
      " ...\n",
      " [ 5.07515618e-03 -2.52415790e-03  3.73340699e-03 ... -3.04961669e-03\n",
      "  -2.34854350e-02 -2.97858769e-03]\n",
      " [ 2.42093567e-03 -9.81824357e-04 -4.87989972e-05 ...  3.06671234e-03\n",
      "   1.67650380e-03 -9.33474072e-05]\n",
      " [ 4.05316111e-03 -8.72758525e-04 -2.13727686e-03 ...  1.71411697e-03\n",
      "   4.82532446e-03  5.42189870e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(lsa_model.components_.shape) # (no_of_topics*no_of_words)\n",
    "print(lsa_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (956110048.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [20]\u001b[1;36m\u001b[0m\n\u001b[1;33m    data = f.read(100)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# most important words for each topic\n",
    "vocab = vect.get_feature_names()\n",
    "\n",
    "for i, comp in enumerate(lsa_model.components_):\n",
    "    vocab_comp = zip(vocab, comp)\n",
    "    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:15]\n",
    "    top_terms_list=list(dict(sorted_words).keys())\n",
    "    wordcloud = WordCloud(width = 1000, height = 500).generate((\" \").join(top_terms_list))\n",
    "    with open(\"topics.txt\", \"a+\") as f:\n",
    "        f.seek(0)\n",
    "        # If file is not empty then append '\\n'\n",
    "        data = f.read(100)\n",
    "        if len(data) > 0 :\n",
    "            f.write(\"\\n\")\n",
    "        # Append text at the end of file\n",
    "        f.write(\"Topic \"+str(i)+\": \",top_terms_list)\n",
    "    print(\"Topic \"+str(i)+\": \",top_terms_list)\n",
    "    plt.figure(figsize=(5,3))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    #print(\"Topic \"+str(i)+\": \")\n",
    "    #for t in sorted_words:\n",
    "     #   print(t[0], end=\" \")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lsa_model.components_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
